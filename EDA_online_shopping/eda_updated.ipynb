{
    "notebook": [
      {
        "cell_type": "markdown",
        "source": "# Exploratory Data Analysis for Online Shopping Dataset\n\nThis dataset provides valuable insights into consumer behaviour where statistical and visualisation techniques help to understand customer behaviour and optimise marketing strategies.\n\nThe data contains information about the website activity of users over one year. Each sample represents the user interacting with the website during a shopping session."
      },
      {
        "cell_type": "markdown",
        "source": "# Necessary Imports:"
      },
      {
        "cell_type": "code",
        "source": "import matplotlib\nmatplotlib.use('TkAgg')\nimport matplotlib.pyplot as plt\n\nimport numpy as np\nimport pandas as pd\nimport plotly\nimport plotly.express as px\nfrom scipy import stats\nimport seaborn as sns\nimport yaml\n\nfrom sqlalchemy import create_engine, text\nfrom sqlalchemy.pool import QueuePool"
      },
      {
        "cell_type": "markdown",
        "source": "# The below function loads in the credentials from the private credentials.yaml file (a part of .gitignore file)."
      },
      {
        "cell_type": "code",
        "source": "from db_utils import load_credentials"
      },
      {
        "cell_type": "code",
        "source": "credentials = load_credentials('credentials.yaml')\nprint(credentials)"
      },
      {
        "cell_type": "markdown",
        "source": "# The online shopping data is stored in an AWS RDS database. The below class includes methods to extract the data from this database.\n#\n# RDSDatabaseConnector Class uses credentials to initialize a database, fetch data and then save to csv.\n#\n# Methods:\n# * _create_connection_string(): creates connection string from credentials\n# * initialize_engine(): generates connection string and initializes engine\n# * fetch_data(query): fetches data from the database\n# * save_data_to_csv(df, file_name): saves dataframe to csv file"
      },
      {
        "cell_type": "code",
        "source": "from db_utils import RDSDatabaseConnector"
      },
      {
        "cell_type": "code",
        "source": "db_connector = RDSDatabaseConnector(credentials) # initializes RDSDatabaseConnector with imported credentials\n\n# try statement tests connection with a simple query of SELECT 1\ntry:\n    db_connector.initialize_engine()\n    print('Engine successfully initialised.')\n    \n    with db_connector.engine.connect() as connection: \n        result = connection.execute(text('SELECT 1'))\n        print('Test query result:', result.fetchone())\n    \n    df = db_connector.fetch_data()\n    print('Fetched data:', df)\n    \n    db_connector.save_data_to_csv(df, 'customer_activity_data.csv') # saves imported data as csv \n\nexcept Exception as e:\n    print(f'An error occurred: {e}')"
      },
      {
        "cell_type": "code",
        "source": "customer_df = pd.read_csv('customer_activity_data.csv') # loads in customer shopping data from csv as pandas dataframe\nprint(customer_df)\n\nshape = customer_df.shape # looks at shape of data \nprint(f'This data has {shape[0]} rows and {shape[1]} columns.')\n\nprint(customer_df.info()) # prints dataframe information"
      },
      {
        "cell_type": "markdown",
        "source": "# The next part covers part of the exploratory data analysis to understand the data and find any patterns. \n# It also handles any missing or incorrectly formatted data."
      },
      {
        "cell_type": "code",
        "source": "from db_utils import DataTransform"
      },
      {
        "cell_type": "markdown",
        "source": "# Four columns were initially object type columns - these were better as category columns as there were a set number of category options. This allowed easier filtering of data."
      },
      {
        "cell_type": "code",
        "source": "transformed_data_types = DataTransform(customer_df)\ntransformed_data_types.object_to_category_transform(['operating_systems', 'traffic_type', 'browser', 'visitor_type'])\n\nupdated_data_types = transformed_data_types.get_transformed_df()\nprint(updated_data_types.info()) # checks columns have been correctly transformed"
      },
      {
        "cell_type": "markdown",
        "source": "# The next section uses methods from the class DataFrameInfo to get information about the updated dataset i.e. statistical summaries and missing values for each column."
      },
      {
        "cell_type": "code",
        "source": "from db_utils import DataFrameInfo"
      },
      {
        "cell_type": "code",
        "source": "df_info = DataFrameInfo()\nprint('Dataframe information:')\ndf_info.get_data_types(updated_data_types)\nprint('Dataframe statistical summary:')\ndf_info.statistical_summary(updated_data_types)\nprint('Dataframe unique values:')\ndf_info.unique_values(updated_data_types)\nprint('Dataframe shape:')\ndf_info.df_shape(updated_data_types)"
      },
      {
        "cell_type": "markdown",
        "source": "# There are 100 rows of data and 17 columns. Administrative, informational, bounce_rates, page_values columns are mostly zeros, product_related columns have lots of unique values. Categorical columns have less unique values as there are set amount of options."
      },
      {
        "cell_type": "markdown",
        "source": "# The DataFrameTransform class contains methods for data transformation based on missing or skewed data: \n# * calc_null_percentage(): calculates percentage of null values in each column of dataframe\n# * impute_missing_values(): imputes missing values based on column skewness\n# * log_skew_transform(skewed_columns): applies best transformation to reduce skewness (log, sqrt, boxcox) in given column\n# * get_transformed_df(): gets transformed dataframe"
      },
      {
        "cell_type": "code",
        "source": "from db_utils import DataFrameTransform"
      },
      {
        "cell_type": "code",
        "source": "df = updated_data_types\ndf_transformer = DataFrameTransform(df)\n\nbefore_null = df.isnull().sum()\nprint('Before Null:\\n', before_null)\n\nnull_percentage = df_transformer.calc_null_percentage()\nprint('Null percentage:\\n', null_percentage)"
      },
      {
        "cell_type": "markdown",
        "source": "# The above results show how many null values there are for each column as a count and a percentage (which is the same as there are 100 rows). Missing values will then be imputed based on their skew: \n# * the value will be imputed with the median of the column if there is positive skew and the mean if there is negative \n# * transformation based on log, sqrt or boxcox is carried out for each missing value\n# \n# The Plotter class has 2 methods for plotting the data: \n# * plot_null_comparison(before_null, after_null, title=\"Null values pre and post imputation\"): plots comparison of null values percentage before and after data imputation\n# * plot_histogram(column_data, title): plots histograms for given column\n# \n# The FindSkew class identifies skewed columns and plots figures for them with 2 methods: \n# * identify_skew(threshold=0.5): identifies skewed columns based on threshold of 0.5\n# * skew_hist(skew_columns, plotter): plots histograms for skewed columns\n# \n# Plots will open up in a new window."
      },
      {
        "cell_type": "code",
        "source": "from db_utils import Plotter\nfrom db_utils import FindSkew"
      },
      {
        "cell_type": "code",
        "source": "df_transformer.impute_missing_values()\nafter_null = df_transformer.get_transformed_df().isnull().sum()\nprint('After Null:\\n',after_null)\n\nplotter = Plotter()\nplotter.plot_null_comparison(before_null, after_null)"
      },
      {
        "cell_type": "markdown",
        "source": "# There were 5 columns with missing data, all imputed with the median due to a positive skew.  The below will plot histograms of each skewed column and again for the updated transformed column and save the transformed dataframe to another csv file. \n# Please note you must close out of each histogram for the next to pop up."
      },
      {
        "cell_type": "code",
        "source": "skew_analysis = FindSkew(df_transformer.get_transformed_df())\nskew_columns = skew_analysis.identify_skew(threshold=0.5)\nprint(f'Skewed Columns: \\n{skew_columns}')\nskew_analysis.skew_hist(skew_columns, plotter)"
      },
      {
        "cell_type": "code",
        "source": "df_transformer.log_skew_transform(skew_columns.index)\ntransformed_df = df_transformer.get_transformed_df()\n\nprint('Plotting updated histograms for transformed columns..')\nfor column in skew_columns.index:\n    plotter.plot_histogram(transformed_df[column], title=f'Updated histogram for {column}')\n\ntransformed_df.to_csv('transformed_customer_activity.csv', index=False)"
      },
      {
        "cell_type": "markdown",
        "source": "# There were 11 skewed columns based on the threshold of 0.5. Boxcox and log transformations were most popular with 4 transformed columns reaching below the skew threshold. \n# Boolean columns were ignored as they were unable to be transformed."
      },
      {
        "cell_type": "markdown",
        "source": "# Running the below code will allow you to see boxplots for the transformed dataframe - an alternative visual to histograms."
      },
      {
        "cell_type": "code",
        "source": "from db_utils import box_plots\n\nbox_plots(transformed_df)"
      },
      {
        "cell_type": "markdown",
        "source": "# The below code allows you to detect outliers using the interquartile range of numeric columns - 5 numeric columns contain outliers with the most in the informational and informational_duration columns."
      },
      {
        "cell_type": "code",
        "source": "from db_utils import detect_outliers_iqr\n\ncolumns_to_exclude = ['month', 'operating_systems', 'browser', 'region', 'traffic_type', 'visitor_type', 'weekend', 'revenue']\nnumeric_data = transformed_df.drop(columns=columns_to_exclude)\n   \noutliers = detect_outliers_iqr(numeric_data)\nfor col, outlier_vals in outliers.items():\n    print(f'{col}: {len(outlier_vals)} outliers detected.')\n\ncolumns_with_outliers = [col for col, vals in outliers.items() if len(vals) > 0]"
      },
      {
        "cell_type": "markdown",
        "source": "# Further transformations are required for the columns with outliers to see if it improves skew in the data, dropping any non-numeric columns."
      },
      {
        "cell_type": "code",
        "source": "transformer_2 = DataFrameTransform(transformed_df)\ntransformations_2 = transformer_2.log_skew_transform(columns_with_outliers)\n\ntransformed_df_2 = transformer_2.get_transformed_df()\nnumeric_data_transformed = transformed_df_2.drop(columns=columns_to_exclude)\n\nprint('Transformations applied to the outlier columns successfully.')"
      },
      {
        "cell_type": "markdown",
        "source": "# The next section visualizes various analysis of the dataset to explore customer traffic patterns and consumer habits."
      },
      {
        "cell_type": "code",
        "source": "# You can explore additional analyses here and visualize data like daily sales, weekday/weekend conversion, and more using plotly or seaborn visualizations."
      }
    ]
  }
  